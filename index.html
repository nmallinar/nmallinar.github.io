<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Neil Rohit Mallinar


</title>
<meta name="description" content="My research, music, comedy, etc.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>♾</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Neil</span> Rohit Mallinar
    </h1>
     <p class="desc">UC San Diego. Halıcıoğlu Data Science Institute. <a href="mailto:nmallina@ucsd.edu">nmallina@ucsd.edu</a>.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        


<img class="img-fluid z-depth-1 rounded" src="/assets/resized/neilmallinar-1400x1050.jpg" srcset="    /assets/resized/neilmallinar-480x360.jpg 480w,    /assets/resized/neilmallinar-800x600.jpg 800w,    /assets/resized/neilmallinar-1400x1050.jpg 1400w,/ assets/img/neilmallinar.jpg 3000w">

      
      
        <div class="address">
          <p>Currently seeking an office :)</p> <p>10100 Hopkins Dr</p> <p>San Diego, CA 92093</p>

        </div>
      
    </div>
    

    <div class="clearfix">
      <p>I’m pursuing my PhD at UC San Diego, advised by <a href="http://misha.belkin-wang.org/" target="_blank" rel="noopener noreferrer">Misha Belkin</a>.
I am also a Research Fellow at The Broad Institute where I am adivsed by <a href="https://www.broadinstitute.org/bios/anthony-philippakis-0" target="_blank" rel="noopener noreferrer">Anthony Philippakis</a>.
For a full list of publications, see <a href="https://scholar.google.com/citations?view_op=list_works&amp;hl=en&amp;authuser=1&amp;hl=en&amp;user=6ogHsLsAAAAJ&amp;sortby=pubdate&amp;authuser=1" target="_blank" rel="noopener noreferrer">Google Scholar</a>.</p>

<p>My research focuses on understanding deep learning, both theoretically and practically. In the past, I studied: benign overfitting in neural networks and kernel regression; and calibration in neural networks as it relates to generalization. My current research focuses on spectral properties of De Bruijn graphs, high-dimensional linear regression under distribution shifts, and, most recently, studying phenomena such as grokking and in-context learning with transformers, fully connected networks, and kernels.</p>

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

    </div>

    
      <div class="news">
  <h2>my life in four days</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Sep 28, 2020</th>
          <td>
            
              Started my PhD at UC San Diego

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 1, 2019</th>
          <td>
            
              Joined Pryon Inc in Brooklyn

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 23, 2016</th>
          <td>
            
              Moved to New York, started at IBM Watson

<!-- inline: long title -->
<!-- Announcements and news can be much longer than just quick inline posts. In fact, they can have all the features available for the standard blog posts. See below.

***

Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.

#### Hipster list
<ul>
    <li>brunch</li>
    <li>fixie</li>
    <li>raybans</li>
    <li>messenger bag</li>
</ul>

Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90's yr typewriter selfies letterpress cardigan vegan.

***

Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.

> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
> —Anais Nin

Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual. -->

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 18, 2016</th>
          <td>
            
              Got my Bachelor’s from Johns Hopkins University

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mallinar-etal-2022-tempered" class="col-sm-8">
    
      <div class="title"><a href="https://arxiv.org/pdf/2207.06569" target="_blank" rel="noopener noreferrer">Benign, tempered, or catastrophic: A taxonomy of overfitting</a></div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Mallinar, Neil</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Simon, James B,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Abedsoltan, Amirhesam,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Pandit, Parthe,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Belkin, Mikhail,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Preetum, Nakkiran
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 36th Conference on Neural Information Processing Systems (NeurIPS 2022)</em>
      
      
        Jul
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods, which perfectly fit their training data. Certain interpolating methods, including neural networks, can fit noisy training data without catastrophically bad test performance, in defiance of standard intuitions from statistical learning theory. Aiming to explain this, a body of recent work has studied \textitbenign overfitting, a phenomenon where some interpolating methods approach Bayes optimality, even in the presence of noise. In this work we argue that while benign overfitting has been instructive and fruitful to study, many real interpolating methods like neural networks \textitdo not fit benignly: modest noise in the training set causes nonzero (but non-infinite) excess risk at test time, implying these models are neither benign nor catastrophic but rather fall in an intermediate regime. We call this intermediate regime \textittempered overfitting, and we initiate its systematic study. We first explore this phenomenon in the context of kernel (ridge) regression (KR) by obtaining conditions on the ridge parameter and kernel eigenspectrum under which KR exhibits each of the three behaviors. We find that kernels with powerlaw spectra, including Laplace kernels and ReLU neural tangent kernels, exhibit tempered overfitting. We then empirically study deep neural networks through the lens of our taxonomy, and find that those trained to interpolation are tempered, while those stopped early are benign. We hope our work leads to a more refined understanding of overfitting in modern learning</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="carrell-etal-2022-calibration" class="col-sm-8">
    
      <div class="title"><a href="https://arxiv.org/pdf/2210.01964" target="_blank" rel="noopener noreferrer">The Calibration Generalization Gap</a></div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Carrell, Annabelle,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Mallinar, Neil</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Lucas, James,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Nakkiran, Preetum
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>2022 International Conference on Machine Learning (Workshop on Distribution-Free Uncertainty Quantification)</em>
      
      
        Oct
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Calibration is a fundamental property of a good predictive model: it requires that the model predicts correctly in proportion to its confidence. Modern neural networks, however, provide no strong guarantees on their calibration – and can be either poorly calibrated or well-calibrated depending on the setting. It is currently unclear which factors contribute to good calibration (architecture, data augmentation, overparameterization, etc), though various claims exist in the literature. We propose a systematic way to study the calibration error: by decomposing it into (1) calibration error on the train set, and (2) the calibration generalization gap. This mirrors the fundamental decomposition of generalization. We then investigate each of these terms, and give empirical evidence that (1) DNNs are typically always calibrated on their train set, and (2) the calibration generalization gap is upper-bounded by the standard generalization gap. Taken together, this implies that models with small generalization gap (|Test Error - Train Error|) are well-calibrated. This perspective unifies many results in the literature, and suggests that interventions which reduce the generalization gap (such as adding data, using heavy augmentation, or smaller model size) also improve calibration. We thus hope our initial study lays the groundwork for a more systematic and comprehensive understanding of the relation between calibration, generalization, and optimization.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="rennie-etal-2020-unsupervised" class="col-sm-8">
    
      <div class="title"><a href="https://aclanthology.org/2020.emnlp-main.87" target="_blank" rel="noopener noreferrer">Unsupervised Adaptation of Question Answering Systems via Generative Self-training</a></div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Rennie, Steven,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Marcheret, Etienne,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Mallinar, Neil</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Nahamoo, David,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Goel, Vaibhava
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
      
      
        Nov
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data. Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance. While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention. In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation. Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data. By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="chen_big-little_2019" class="col-sm-8">
    
      <div class="title"><a href="http://arxiv.org/abs/1807.03848" target="_blank" rel="noopener noreferrer">Big-Little Net: An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition</a></div>
      <div class="author">
        
          
          
          
          

          
            
              
                
                  Chen, Chun-Fu,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Fan, Quanfu,
                
              
            
          
        
          
          
          
          

          
            
              
                <em>Mallinar, Neil</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Sercu, Tom,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Feris, Rogerio
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>International Conference on Learning Representations</em>
      
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33% on object recognition while improving accuracy with 0.9%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mallinar_bootstrapping_2018" class="col-sm-8">
    
      <div class="title"><a href="http://arxiv.org/abs/1812.06176" target="_blank" rel="noopener noreferrer">Bootstrapping Conversational Agents With Weak Supervision</a></div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Mallinar, Neil</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Shah, Abhishek,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Ugrani, Rajendra,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Gupta, Ayush,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Gurusankar, Manikandan,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Ho, Tin Kam,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Liao, Q. Vera,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Zhang, Yunfeng,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Bellamy, Rachel K. E.,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Yates, Robert,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  Desmarais, Chris,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and McGregor, Blake
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>
      
      
        Dec
      
      
        2018
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Many conversational agents in the market today follow a standard bot development framework which requires training intent classifiers to recognize user input. The need to create a proper set of training examples is often the bottleneck in the development process. In many occasions agent developers have access to historical chat logs that can provide a good quantity as well as coverage of training examples. However, the cost of labeling them with tens to hundreds of intents often prohibits taking full advantage of these chat logs. In this paper, we present a framework called }textit{search, label, and propagate} (SLP) for bootstrapping intents from existing chat logs using weak supervision. The framework reduces hours to days of labeling effort down to minutes of work by using a search engine to find examples, then relies on a data programming approach to automatically expand the labels. We report on a user study that shows positive user feedback for this new approach to build conversational agents, and demonstrates the effectiveness of using data programming for auto-labeling. While the system is developed for training conversational agents, the framework has broader application in significantly reducing labeling effort for training text classifiers.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="mallinar_probabilistic_2017" class="col-sm-8">
    
      <div class="title"><a href="http://arxiv.org/abs/1706.09546" target="_blank" rel="noopener noreferrer">Probabilistic Cross-Identification of Galaxies with Realistic Clustering</a></div>
      <div class="author">
        
          
          
          
          

          
            
              
                <em>Mallinar, Neil</em>,
              
            
          
        
          
          
          
          

          
            
              
                
                  Budavari, Tamas,
                
              
            
          
        
          
          
          
          

          
            
              
                
                  and Lemson, Gerard
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Astronomy and Computing</em>
      
      
        Jul
      
      
        2017
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Probabilistic cross-identification has been successfully applied to a number of problems in astronomy from matching simple point sources to associating stars with unknown proper motions and even radio observations with realistic morphology. Here we study the Bayes factor for clustered objects and focus in particular on galaxies to assess the effect of typical angular correlations. Numerical calculations provide the modified relationship, which (as expected) suppresses the evidence for the associations at the shortest separations where the 2-point auto-correlation function is large. Ultimately this means that the matching probability drops at somewhat shorter scales than in previous models.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6E%6D%61%6C%6C%69%6E%61%72@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=6ogHsLsAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/nmallinar" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/nmallinar" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/nmallinar" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>







<a href="https://gitlab.com/nmallinar" title="GitLab" target="_blank" rel="noopener noreferrer"><i class="fab fa-gitlab"></i></a>





      </div>
      <div class="contact-note">Feel free to reach out to me about anything!
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2023 Neil Rohit Mallinar.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
