---
---

@article{Mallinar-etal-2025-scalable,
  title="A Scalable Framework for Evaluating Health Language Models",
  author={Mallinar, Neil and A Ali Heydari and Xin Liu and Anthony Z Faranesh and Brent Winslow and Nova Hammerquist and Benjamin Graef and Cathy Speed and Mark Malhotra and Shwetak Patel and Javier L Prieto and Daniel McDuff and Ahmed A Metwally},
  journal="arXiv preprint",
  abstract={Large language models (LLMs) have emerged as powerful tools for analyzing complex datasets. Recent studies demonstrate their potential to generate useful, personalized responses when provided with patient-specific health information that encompasses lifestyle, biomarkers, and context. As LLM-driven health applications are increasingly adopted, rigorous and efficient one-sided evaluation methodologies are crucial to ensure response quality across multiple dimensions, including accuracy, personalization and safety. Current evaluation practices for open-ended text responses heavily rely on human experts. This approach introduces human factors and is often cost-prohibitive, labor-intensive, and hinders scalability, especially in complex domains like healthcare where response assessment necessitates domain expertise and considers multifaceted patient data. In this work, we introduce Adaptive Precise Boolean rubrics\: an evaluation framework that streamlines human and automated evaluation of open-ended questions by identifying gaps in model responses using a minimal set of targeted rubrics questions. Our approach is based on recent work in more general evaluation settings that contrasts a smaller set of complex evaluation targets with a larger set of more precise, granular targets answerable with simple boolean responses. We validate this approach in metabolic health, a domain encompassing diabetes, cardiovascular disease, and obesity. Our results demonstrate that Adaptive Precise Boolean rubrics yield higher inter-rater agreement among expert and non-expert human evaluators, and in automated assessments, compared to traditional Likert scales, while requiring approximately half the evaluation time of Likert-based methods. This enhanced efficiency, particularly in automated evaluation and non-expert contributions, paves the way for more extensive and cost-effective evaluation of LLMs in health.},
  url="https://arxiv.org/abs/2503.23339",
  month=mar,
  year="2025",
  selected = {true}
}

@article{mallinar2024emergencenonneuralmodelsgrokking,
      title={Emergence in non-neural models: grokking modular arithmetic via average gradient outer product},
      author={Neil Mallinar and Daniel Beaglehole and Libin Zhu and Adityanarayanan Radhakrishnan and Parthe Pandit and Mikhail Belkin},
      abstract={Neural networks trained to solve modular arithmetic tasks exhibit grokking, a phenomenon where the test accuracy starts improving long after the model achieves 100\% training accuracy in the training process. It is often taken as an example of 'emergence', where model ability manifests sharply through a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural networks nor to gradient descent-based optimization. Specifically, we show that this phenomenon occurs when learning modular arithmetic with Recursive Feature Machines (RFM), an iterative algorithm that uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with general machine learning models. When used in conjunction with kernel machines, iterating RFM results in a fast transition from random, near zero, test accuracy to perfect test accuracy. This transition cannot be predicted from the training loss, which is identically zero, nor from the test loss, which remains constant in initial iterations. Instead, as we show, the transition is completely determined by feature learning: RFM gradually learns block-circulant features to solve modular arithmetic. Paralleling the results for RFM, we show that neural networks that solve modular arithmetic also learn block-circulant features. Furthermore, we present theoretical evidence that RFM uses such block-circulant features to implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution neural networks learn on these tasks. Our results demonstrate that emergence can result purely from learning task-relevant features and is not specific to neural architectures nor gradient descent-based optimization methods. Furthermore, our work provides more evidence for AGOP as a key mechanism for feature learning in neural networks.},
      year={2024},
      month=oct,
      selected={true},
      journal="arXin preprint",
      url={https://arxiv.org/abs/2407.20199},
}

@article{Philippakis-etal-2024-debruijn,
  title="Eigenvectors of the De Bruijn Graph Laplacian: A Natural Basis for the Cut and Cycle Space",
  author="Philippakis, Anthony and Mallinar, Neil and Pandit, Parthe and Belkin, Mikhail",
  journal="arXiv preprint",
  abstract="We study the Laplacian of the undirected De Bruijn graph over an alphabet  of order . While the eigenvalues of this Laplacian were found in 1998 by Delorme and Tillich [1], an explicit description of its eigenvectors has remained elusive. In this work, we find these eigenvectors in closed form and show that they yield a natural and canonical basis for the cut- and cycle-spaces of De Bruijn graphs. Remarkably, we find that the cycle basis we construct is a basis for the cycle space of both the undirected and the directed De Bruijn graph. This is done by developing an analogue of the Fourier transform on the De Bruijn graph, which acts to diagonalize the Laplacian. Moreover, we show that the cycle-space of De Bruijn graphs, when considering all possible orders of  simultaneously, contains a rich algebraic structure, that of a graded Hopf algebra.",
  url="https://arxiv.org/abs/2410.07622",
  month=oct,
  year="2024",
  selected = {true}
}

@inproceedings{mallinar-etal-2024-minimum,
    title = "Minimum-Norm Interpolation Under Covariate Shift",
    author="Neil Mallinar* and Austin Zane* and Frei, Spencer and Yu, Bin",
    booktitle = "Proceedings of the 41st International Conference on Machine Learning (ICML 2024)",
    month = mar,
    year = "2024",
    address = "Online",
    url = "https://arxiv.org/abs/2404.00522",
    abstract = "Transfer learning is a critical part of real-world machine learning deployments and has been extensively studied in experimental works with overparameterized neural networks. However, even in the simplest setting of linear regression a notable gap still exists in the theoretical understanding of transfer learning. In-distribution research on high-dimensional linear regression has led to the identification of a phenomenon known as \textit{benign overfitting}, in which linear interpolators overfit to noisy training labels and yet still generalize well. This behavior occurs under specific conditions on the source covariance matrix and input data dimension. Therefore, it is natural to wonder how such high-dimensional linear models behave under transfer learning. We prove the first non-asymptotic excess risk bounds for benignly-overfit linear interpolators in the transfer learning setting. From our analysis, we propose a taxonomy of \textit{beneficial} and \textit{malignant} covariate shifts based on the degree of overparameterization. We follow our analysis with empirical studies that show these beneficial and malignant covariate shifts for linear interpolators on real image data, and for fully-connected neural networks in settings where the input data dimension is larger than the training sample size.",
    selected = {true}
}

@inproceedings{mallinar-etal-2022-tempered,
    title = "Benign, tempered, or catastrophic: A taxonomy of overfitting",
    author="Neil Mallinar* and James B Simon* and Abedsoltan, Amirhesam and Pandit, Parthe and Belkin, Mikhail and Nakkiran Preetum",
    booktitle = "36th Conference on Neural Information Processing Systems (NeurIPS 2022)",
    month = jul,
    year = "2022",
    address = "Online",
    url = "https://arxiv.org/pdf/2207.06569",
    abstract = "The practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods, which perfectly fit their training data. Certain interpolating methods, including neural networks, can fit noisy training data without catastrophically bad test performance, in defiance of standard intuitions from statistical learning theory. Aiming to explain this, a body of recent work has studied $\textit{benign overfitting}$, a phenomenon where some interpolating methods approach Bayes optimality, even in the presence of noise. In this work we argue that while benign overfitting has been instructive and fruitful to study, many real interpolating methods like neural networks $\textit{do not fit benignly}$: modest noise in the training set causes nonzero (but non-infinite) excess risk at test time, implying these models are neither benign nor catastrophic but rather fall in an intermediate regime. We call this intermediate regime $\textit{tempered overfitting}$, and we initiate its systematic study. We first explore this phenomenon in the context of kernel (ridge) regression (KR) by obtaining conditions on the ridge parameter and kernel eigenspectrum under which KR exhibits each of the three behaviors. We find that kernels with powerlaw spectra, including Laplace kernels and ReLU neural tangent kernels, exhibit tempered overfitting. We then empirically study deep neural networks through the lens of our taxonomy, and find that those trained to interpolation are tempered, while those stopped early are benign. We hope our work leads to a more refined understanding of overfitting in modern learning",
    selected = {true}
}


@article{carrell-etal-2022-calibration,
  title="The Calibration Generalization Gap",
  author="Carrell, Annabelle and Mallinar, Neil and Lucas, James and Nakkiran, Preetum",
  journal="2022 International Conference on Machine Learning (Workshop on Distribution-Free Uncertainty Quantification)",
  abstract="Calibration is a fundamental property of a good predictive model: it requires that the model predicts correctly in proportion to its confidence. Modern neural networks, however, provide no strong guarantees on their calibration -- and can be either poorly calibrated or well-calibrated depending on the setting. It is currently unclear which factors contribute to good calibration (architecture, data augmentation, overparameterization, etc), though various claims exist in the literature. We propose a systematic way to study the calibration error: by decomposing it into (1) calibration error on the train set, and (2) the calibration generalization gap. This mirrors the fundamental decomposition of generalization. We then investigate each of these terms, and give empirical evidence that (1) DNNs are typically always calibrated on their train set, and (2) the calibration generalization gap is upper-bounded by the standard generalization gap. Taken together, this implies that models with small generalization gap (|Test Error - Train Error|) are well-calibrated. This perspective unifies many results in the literature, and suggests that interventions which reduce the generalization gap (such as adding data, using heavy augmentation, or smaller model size) also improve calibration. We thus hope our initial study lays the groundwork for a more systematic and comprehensive understanding of the relation between calibration, generalization, and optimization.",
  url="https://arxiv.org/pdf/2210.01964",
  month=oct,
  year="2022",
  selected = {false}
}

@inproceedings{rennie-etal-2020-unsupervised,
    title = "Unsupervised Adaptation of Question Answering Systems via Generative Self-training",
    author = "Rennie, Steven  and
      Marcheret, Etienne  and
      Mallinar, Neil  and
      Nahamoo, David  and
      Goel, Vaibhava",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.87",
    doi = "10.18653/v1/2020.emnlp-main.87",
    pages = "1148--1157",
    abstract = "BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks. These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data. Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance. While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention. In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation. Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data. By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.",
    selected = {false}
}


@article{mallinar_iterative_2020,
  title = {Iterative {Data} {Programming} for {Expanding} {Text} {Classification} {Corpora}},
  volume = {34},
  copyright = {All rights reserved},
  url = {http://arxiv.org/abs/2002.01412},
  abstract = {Real-world text classification tasks often require many labeled training examples that are expensive to obtain. Recent advancements in machine teaching, specifically the data programming paradigm, facilitate the creation of training data sets quickly via a general framework for building weak models, also known as labeling functions, and denoising them through ensemble learning techniques. We present a fast, simple data programming method for augmenting text data sets by generating neighborhood-based weak models with minimal supervision. Furthermore, our method employs an iterative procedure to identify sparsely distributed examples from large volumes of unlabeled data. The iterative data programming techniques improve newer weak models as more labeled data is confirmed with human-in-loop. We show empirical results on sentence classification tasks, including those from a task of improving intent recognition in conversational agents.},
  urldate = {2020-02-06},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author = {Mallinar, Neil and Shah, Abhishek and Ho, Tin Kam and Ugrani, Rajendra and Gupta, Ayush},
  month = feb,
  year = {2020},
  note = {arXiv: 2002.01412},
  selected = {false},
  annote = {Comment: 6 pages, 2 figures, In Proceedings of the AAAI Conference on Artificial Intelligence 2020 (IAAI Technical Track: Emerging Papers)},
  annote = {Extracted Annotations (2/6/2020, 5:05:14 PM)
"It is even more challenging when the classes are highly skewed, which renders a sequential process to look for each class largely ineffective, as examples of the minority class are buried in much larger amounts of data of other classes." (Mallinar et al 2020:1)
"Guided learning approaches aim to lessen the amount of data to be labeled, by using search to select relevant examples for labeling, and are shown to be more effective than random labeling and active learning techniques in settings where data exhibits high skew" (Mallinar et al 2020:2)
"We distinguish our work from active learning frameworks as in (Settles 2009) by that we never access or query the" (Mallinar et al 2020:2)
"downstream learner during our procedure. Often such learners require a decent amount of data and tuning on validation sets to be effective models. In our setting, the amount of labeled data provided at the start is very limited ({\textless} 20 examples per class) to the point that even a validation set is impossible to extract." (Mallinar et al 2020:3)
"Our procedure repeats a basic step that consists of leveraging the labeled examples to rank and select the unlabeled examples, and then consulting an oracle for labeling the selected ones. In each step the selection is limited in size to minimize consumption of the oracle resources. This procedure continues until a termination criteria is reached." (Mallinar et al 2020:3)
"For multi-class data, we simply apply the same expansion process independently for each class and merge the resultant positive labeled sets to get our final expanded training set to avoid extreme data skew that might be caused by intermediate steps prior to the completion of expansion of all the classes." (Mallinar et al 2020:3)
"The labeler can be expensive to evaluate in practice (often a domain expert), so we include a batch size setting that restrict the periteration labeling effort, and a termination criteria to determine when to stop expanding the data." (Mallinar et al 2020:3)
"This is repeated until a termination criteria, parameterized by t, is met. We define t to be the maximum number of consecutive examples we are willing to send to the labeler, in each iteration, without receiving any positive labels back. We consider the data to be fully expanded when we reach this point. In this work, we use t = 30 and b = 10 to simulate a user looking through three pages of relevant examples with ten examples per page before quitting." (Mallinar et al 2020:3)
"Here we use the "More Like This" (MLT) feature of the search engine Elasticsearch 1 to expand batches of examples and rank them based on the returned score. As such, this selection model only uses positively labeled data for expansion and does not make use of negative labels." (Mallinar et al 2020:3)
"We note that while this work uses lexical matching to build neighborhoods, the way we construct weak models can be generalized to any reasonable way to define a subset in a metric space." (Mallinar et al 2020:4)
"In the medium density setting, a probabilistic generative model p (; Y ) is trained over and the true labels, Y , of the examples. It is shown that in the medium density setting, such a model can better denoise the assignments between weak models than a simple majority vote." (Mallinar et al 2020:4)
"We construct weak models with two independent MLT expansions on the newly labeled positive and negative examples obtained at each iteration." (Mallinar et al 2020:4)
"This automatic construction of weak models eliminates the need for domain experts to encode relevant heuristics about the classes, or the need for many crowdsourced workers. Instead, a single labeler can quickly label a small number of examples over a few iterations and generate a larger label matrix, ." (Mallinar et al 2020:4)
"After training the generative model, we finally retrieve a list of computed probabilistic labels over examples in our unlabeled corpus (where we default to a score of 0 for those examples that are not covered by at least one weak model). These probabilistic labels are used to rank examples for selection in future iterations of Algorithm 1." (Mallinar et al 2020:4)
"This merging scheme ensures that at each iteration there is an expansion method that reflects the latest qualities of the labeled data (search), as well as an expansion method that produces stronger, less noisy rankings at the cost of being slightly outdated relative to the current set of labeled data (data programming)." (Mallinar et al 2020:4)
"Our experience indicates that most users can afford to hand-write or hand-select a very small set of examples per class. So we hand-select such examples as the initial training set in our experiments." (Mallinar et al 2020:4)
"The final Reddit dataset consists of 810 labeled titles to start with, 63,990 unlabeled titles and 16,200 titles as test set." (Mallinar et al 2020:4)
"The English data set, CS-En, is split into an unlabeled corpus of 15,236 examples, initially labeled set of size 1,998, and test set of size 15,177 spanning 97 categories." (Mallinar et al 2020:4)
"we start with the initial labeled set and expand each class independently as described in Algorithm 1. After the expansion procedure finishes for each class, we combine all of the fully expanded classes together to create the final labeled training data set" (Mallinar et al 2020:5)
"we train a text classifier 3 on this labeled set and evaluate accuracy on the held-out test set. This accuracy is called the terminal accuracy for the given method used in expansion. We compare the terminal accuracy to the optimal accuracy, which is obtained by training the same classifier on using the ground truth for the entire corpus from which the examples are selected, and evaluating on the test set." (Mallinar et al 2020:5)
"apid ascent than I-MLT, and are able to continue to find useful examples for many more rounds before hitting the termination condition. This can be seen more clearly for the class "videogame controller", where I-MLT seems to converge earlier and reach a recall asymptote whereas I-DP, at the same number of examples sent for labeling, continues to expand." (Mallinar et al 2020:6)
"For instance, the method assumes that the full scope of a class can be reached through chains of neighbors. When this is not true with heavily fragmented classes, we need alternative ways to scatter some seeds to every fragment, by randomization or in-depth analysis of the data characteristics." (Mallinar et al 2020:6)},
  file = {Mallinar et al_2020_Iterative Data Programming for Expanding Text Classification Corpora.pdf:/Users/nmallinar/Google Drive/Research/ZoteroSync/Mallinar et al_2020_Iterative Data Programming for Expanding Text Classification Corpora.pdf:application/pdf},
}

@article{chen_big-little_2019,
  title = {Big-{Little} {Net}: {An} {Efficient} {Multi}-{Scale} {Feature} {Representation} for {Visual} and {Speech} {Recognition}},
  copyright = {All rights reserved},
  shorttitle = {Big-{Little} {Net}},
  url = {http://arxiv.org/abs/1807.03848},
  abstract = {In this paper, we propose a novel Convolutional Neural Network (CNN) architecture for learning multi-scale feature representations with good tradeoffs between speed and accuracy. This is achieved by using a multi-branch network, which has different computational complexity at different branches. Through frequent merging of features from branches at distinct scales, our model obtains multi-scale features while using less computation. The proposed approach demonstrates improvement of model efficiency and performance on both object recognition and speech recognition tasks,using popular architectures including ResNet and ResNeXt. For object recognition, our approach reduces computation by 33\% on object recognition while improving accuracy with 0.9\%. Furthermore, our model surpasses state-of-the-art CNN acceleration approaches by a large margin in accuracy and FLOPs reduction. On the task of speech recognition, our proposed multi-scale CNNs save 30\% FLOPs with slightly better word error rates, showing good generalization across domains. The codes are available at https://github.com/IBM/BigLittleNet},
  urldate = {2019-10-04},
  journal = {International Conference on Learning Representations},
  author = {Chen, Chun-Fu and Fan, Quanfu and Mallinar, Neil and Sercu, Tom and Feris, Rogerio},
  year = {2019},
  note = {arXiv: 1807.03848},
  annote = {Comment: git repo: https://github.com/IBM/BigLittleNet},
  file = {Chen et al_2019_Big-Little Net - An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition.pdf:/Users/nmallinar/Google Drive/Research/ZoteroSync/Chen et al_2019_Big-Little Net - An Efficient Multi-Scale Feature Representation for Visual and Speech Recognition.pdf:application/pdf},
  selected = {false}
}

@article{sercu2019multiframe,
      title={Multi-Frame Cross-Entropy Training for Convolutional Neural Networks in Speech Recognition},
      author={Tom Sercu and Neil Mallinar},
      year={2019},
      eprint={1907.13121},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url = {https://arxiv.org/abs/1907.13121},
      journal = {arXiv preprint arXiv:1907.13121},
      abstract = {We introduce Multi-Frame Cross-Entropy training (MFCE) for convolutional neural network acoustic models. Recognizing that similar to RNNs, CNNs are in nature sequence models that take variable length inputs, we propose to take as input to the CNN a part of an utterance long enough that multiple labels are predicted at once, therefore getting cross-entropy loss signal from multiple adjacent frames. This increases the amount of label information drastically for small marginal computational cost. We show large WER improvements on hub5 and rt02 after training on the 2000-hour Switchboard benchmark.}
}

@misc{sercu2019multiframe,
      title={Multi-Frame Cross-Entropy Training for Convolutional Neural Networks in Speech Recognition},
      author={Tom Sercu and Neil Mallinar},
      year={2019},
      eprint={1907.13121},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@article{mallinar_bootstrapping_2018,
  title = {Bootstrapping {Conversational} {Agents} {With} {Weak} {Supervision}},
  volume = {33},
  copyright = {All rights reserved},
  url = {http://arxiv.org/abs/1812.06176},
  abstract = {Many conversational agents in the market today follow a standard bot development framework which requires training intent classifiers to recognize user input. The need to create a proper set of training examples is often the bottleneck in the development process. In many occasions agent developers have access to historical chat logs that can provide a good quantity as well as coverage of training examples. However, the cost of labeling them with tens to hundreds of intents often prohibits taking full advantage of these chat logs. In this paper, we present a framework called {\textbackslash}textit\{search, label, and propagate\} (SLP) for bootstrapping intents from existing chat logs using weak supervision. The framework reduces hours to days of labeling effort down to minutes of work by using a search engine to find examples, then relies on a data programming approach to automatically expand the labels. We report on a user study that shows positive user feedback for this new approach to build conversational agents, and demonstrates the effectiveness of using data programming for auto-labeling. While the system is developed for training conversational agents, the framework has broader application in significantly reducing labeling effort for training text classifiers.},
  urldate = {2019-10-04},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author = {Mallinar, Neil and Shah, Abhishek and Ugrani, Rajendra and Gupta, Ayush and Gurusankar, Manikandan and Ho, Tin Kam and Liao, Q. Vera and Zhang, Yunfeng and Bellamy, Rachel K. E. and Yates, Robert and Desmarais, Chris and McGregor, Blake},
  month = dec,
  year = {2018},
  note = {arXiv: 1812.06176},
  pages = {9528--9533},
  annote = {Comment: 6 pages, 3 figures, 1 table, Accepted for publication in IAAI 2019},
  file = {Mallinar et al_2018_Bootstrapping Conversational Agents With Weak Supervision.pdf:/Users/nmallinar/Google Drive/Research/ZoteroSync/Mallinar et al_2018_Bootstrapping Conversational Agents With Weak Supervision.pdf:application/pdf},
  selected = {false}
}

@article{mallinar_deep_2018,
  title = {Deep {Canonically} {Correlated} {LSTMs}},
  copyright = {All rights reserved},
  url = {http://arxiv.org/abs/1801.05407},
  abstract = {We examine Deep Canonically Correlated LSTMs as a way to learn nonlinear transformations of variable length sequences and embed them into a correlated, fixed dimensional space. We use LSTMs to transform multi-view time-series data non-linearly while learning temporal relationships within the data. We then perform correlation analysis on the outputs of these neural networks to find a correlated subspace through which we get our final representation via projection. This work follows from previous work done on Deep Canonical Correlation (DCCA), in which deep feed-forward neural networks were used to learn nonlinear transformations of data while maximizing correlation.},
  urldate = {2019-10-04},
  journal = {The Johns Hopkins University Bachelors Thesis},
  author = {Mallinar, Neil and Rosset, Corbin},
  month = jan,
  year = {2018},
  note = {arXiv: 1801.05407},
  annote = {Comment: 8 pages, 3 figures, accepted as the undergraduate honors thesis for Neil Mallinar by The Johns Hopkins University},
  file = {Mallinar_Rosset_2018_Deep Canonically Correlated LSTMs.pdf:/Users/nmallinar/Google Drive/Research/ZoteroSync/Mallinar_Rosset_2018_Deep Canonically Correlated LSTMs.pdf:application/pdf},
}

@article{mallinar_probabilistic_2017,
  title = {Probabilistic {Cross}-{Identification} of {Galaxies} with {Realistic} {Clustering}},
  volume = {20},
  copyright = {All rights reserved},
  issn = {22131337},
  url = {http://arxiv.org/abs/1706.09546},
  doi = {10/gb4453},
  abstract = {Probabilistic cross-identification has been successfully applied to a number of problems in astronomy from matching simple point sources to associating stars with unknown proper motions and even radio observations with realistic morphology. Here we study the Bayes factor for clustered objects and focus in particular on galaxies to assess the effect of typical angular correlations. Numerical calculations provide the modified relationship, which (as expected) suppresses the evidence for the associations at the shortest separations where the 2-point auto-correlation function is large. Ultimately this means that the matching probability drops at somewhat shorter scales than in previous models.},
  urldate = {2019-10-04},
  journal = {Astronomy and Computing},
  author = {Mallinar, Neil and Budavari, Tamas and Lemson, Gerard},
  month = jul,
  year = {2017},
  note = {arXiv: 1706.09546},
  pages = {83--86},
  annote = {Comment: Accepted for publication in Astronomy and Computing, 6 pages, 3 figures},
  file = {Mallinar et al_2017_Probabilistic Cross-Identification of Galaxies with Realistic Clustering.pdf:/Users/nmallinar/Google Drive/Research/ZoteroSync/Mallinar et al_2017_Probabilistic Cross-Identification of Galaxies with Realistic Clustering.pdf:application/pdf},
  selected = {false}
}
